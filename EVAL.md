## Parameter-Free (PF) Evaluation: Augmented Slices â†’ Case Studies

This document captures the commands to reproduce training on augmented CFGs and evaluation on case-study CFGs (no overlap), along with where results are saved and the latest summary.

### 1) Prepare datasets

Train on augmented CFGs generated by the pipeline (`cfg_run_1`), test on case-study CFGs (`case_studies_cfg`).

```bash
cd /home/ubuntu/CFWR

# Build split directory
rm -rf test_results/pf_aug_vs_cs
mkdir -p test_results/pf_aug_vs_cs/train/cfg_output
mkdir -p test_results/pf_aug_vs_cs/test/cfg_output

# Train = augmented CFGs; Test = case studies CFGs
rsync -a /home/ubuntu/CFWR/cfg_run_1/ test_results/pf_aug_vs_cs/train/cfg_output/
rsync -a /home/ubuntu/CFWR/case_studies_cfg/ test_results/pf_aug_vs_cs/test/cfg_output/
```

If you have not generated these yet:

```bash
# Augment slices (defaults already set to factor 10 in the pipeline)
python3 pipeline.py --steps cfg --slicer cf --slices_dir /home/ubuntu/CFWR/specimin/slices \
  --augmented_dir /home/ubuntu/CFWR/slices_aug_cf --cfg_output_dir /home/ubuntu/CFWR/cfg_run_1 | cat

# Case studies (Index Checker examples): clone and generate CFGs
mkdir -p case_studies && cd case_studies
git clone --depth 1 https://github.com/mernst/plume-lib.git || true
cd /home/ubuntu/CFWR
python3 - << 'PY'
import os, sys, subprocess
java_root='/home/ubuntu/CFWR/case_studies/plume-lib/java/src'
out_dir='/home/ubuntu/CFWR/case_studies_cfg'
os.makedirs(out_dir, exist_ok=True)
for r,_,fs in os.walk(java_root):
    for f in fs:
        if f.endswith('.java'):
            subprocess.run([sys.executable,'/home/ubuntu/CFWR/cfg.py','--java_file',os.path.join(r,f),'--out_dir',out_dir], check=False)
print('Done building case_studies_cfg')
PY
```

### 2) Run PF evaluation (exclude Bottom)

```bash
cd /home/ubuntu/CFWR
python3 comprehensive_annotation_type_evaluation.py \
  --dataset_dir test_results/pf_aug_vs_cs \
  --parameter_free \
  --exclude_bottom | cat
```

Outputs are saved to:
- `test_results/comprehensive_annotation_type_evaluation/comprehensive_annotation_type_evaluation_results.json`
- `test_results/comprehensive_annotation_type_evaluation/detailed_annotation_type_evaluation_results.json`

### 3) Latest summary (PF, exclude Bottom)

Train: augmented CFGs (`cfg_run_1`)
Test: case studies CFGs (`case_studies_cfg`)

Overall accuracy / F1(weighted):
- AnnotationTypeGBT: 0.275 / 0.163
- AnnotationTypeHGT: 0.275 / 0.119
- AnnotationTypeCausal: 0.253 / 0.102
- SGCFGNet: 0.278 / 0.129
- GCN: 0.253 / 0.152
- DG2N: 0.286 / 0.127
- DG-CRF-lite: 0.215 / 0.142 (smoke test on cfg_run_1)
- GCSN: 0.307 / 0.189 (smoke test on cfg_run_1)

Per-annotation F1 (subset):
- @GTENegativeOne: GBT 0.203, HGT 0.432, SGCFGNet 0.429, GCN 0.069, DG-CRF-lite 0.185, GCSN 0.221
- @NonNegative:    GBT 0.425, HGT 0.000,  Causal 0.404,  GCN 0.154, DG-CRF-lite 0.198, GCSN 0.267
- @Positive:       all near 0 (in this run)

### 4) New Model Performance (Smoke Test Results)

**DG-CRF-lite (Deterministically-Gated Factor Model)**
- **Architecture**: Hard-concrete feature gates + hard class constraint projection
- **Training**: 428 graphs from cfg_run_1, 20 epochs
- **Performance**: 0.215 accuracy, 0.142 F1(weighted)
- **Key Features**: 
  - Deterministic feature selection (0% or 100% relevance)
  - Hard constraints enforce typing rules
  - L0 regularization for sparsity
- **Per-class F1**: @GTENegativeOne 0.185, @NonNegative 0.198

**GCSN (Gated Causal Subgraph Network)**
- **Architecture**: L0 feature gates + edge top-k subgraph selection
- **Training**: 344 train, 42 val, 42 test graphs from cfg_run_1
- **Performance**: 0.307 accuracy, 0.189 F1(weighted)
- **Key Features**:
  - Feature-level L0 gates for relevance selection
  - Edge-level top-k selection for subgraph focus
  - GATv2Conv with edge attention
- **Per-class F1**: @GTENegativeOne 0.221, @NonNegative 0.267

**Model Comparison Notes**:
- GCSN shows higher accuracy (0.307) than DG-CRF-lite (0.215)
- Both new models outperform baseline GCN (0.253 accuracy)
- GCSN's edge selection mechanism appears more effective for PF annotation tasks
- DG-CRF-lite's hard constraints provide interpretable feature selection

See the detailed JSON for complete per-class tables, confusion matrices, and classification reports.


