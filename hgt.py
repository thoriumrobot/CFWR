import os
import json
import random
import subprocess
import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.nn import HGTConv
from torch_geometric.data import HeteroData, DataLoader
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from cfg import generate_control_flow_graphs, save_cfgs

def ensure_cfg(java_file):
    base = os.path.splitext(os.path.basename(java_file))[0]
    out_dir = os.path.join(cfg_output_dir, base)
    if not os.path.exists(out_dir) or not any(name.endswith('.json') for name in os.listdir(out_dir)):
        cfgs = generate_control_flow_graphs(java_file, cfg_output_dir)
        save_cfgs(cfgs, out_dir)

# Directory paths
cfg_output_dir = os.environ.get("CFG_OUTPUT_DIR", "cfg_output")
slices_dir = os.environ.get("SLICES_DIR", "slices")
index_checker_cp = os.environ.get("CHECKERFRAMEWORK_CP", "")
models_dir = os.environ.get("MODELS_DIR", "models")

if not os.path.exists(models_dir):
    os.makedirs(models_dir)

# Function to load CFGs and convert them into HeteroData objects
def iter_java_files(root_dir):
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f.endswith(".java"):
                yield os.path.join(root, f)

def load_data():
    graphs = []
    labels = []

    for java_file_path in iter_java_files(slices_dir):
        # Run Index Checker to get warnings
        warnings = run_index_checker(java_file_path)
        annotations = parse_warnings(warnings)
        # Load CFGs
        ensure_cfg(java_file_path)
        method_cfgs = load_cfgs(java_file_path)
        for cfg_data in method_cfgs:
            # Create HeteroData object
            data = create_heterodata(cfg_data)
            if data is None:
                continue
            # Label nodes based on warnings
            label_nodes(data, cfg_data, annotations)
            graphs.append(data)
    return graphs

def load_cfgs(java_file):
    """
    Load the saved CFGs for a given Java file.
    """
    method_cfgs = []
    java_file_name = os.path.splitext(os.path.basename(java_file))[0]
    cfg_dir = os.path.join(cfg_output_dir, java_file_name)
    if os.path.exists(cfg_dir):
        for cfg_file in os.listdir(cfg_dir):
            if cfg_file.endswith('.json'):
                cfg_file_path = os.path.join(cfg_dir, cfg_file)
                with open(cfg_file_path, 'r') as f:
                    cfg_data = json.load(f)
                    # Add method name to cfg_data for identification
                    cfg_data['method_name'] = os.path.splitext(cfg_file)[0]
                    cfg_data['java_file'] = java_file
                    method_cfgs.append(cfg_data)
    else:
        print(f"CFG directory {cfg_dir} does not exist for Java file {java_file}")
    return method_cfgs

def run_index_checker(java_file):
    """
    Run the Checker Framework's Index Checker on the given Java file and capture warnings.
    """
    # Construct the command to run the Index Checker
    cp = index_checker_cp
    command = ['javac']
    if cp:
        command += ['-cp', cp]
    command += ['-processor', 'org.checkerframework.checker.index.IndexChecker', java_file]
    result = subprocess.run(command, capture_output=True, text=True)
    warnings = result.stderr  # Warnings are typically output to stderr
    return warnings

def parse_warnings(warnings):
    """
    Parse the warnings generated by Index Checker to identify nodes for annotations.
    """
    import re
    pattern = re.compile(r'^(.*\.java):(\d+):\s*(error|warning):\s*(.*)$')
    annotations = []  # List of dictionaries with file, line, and message

    for line in warnings.split('\n'):
        match = pattern.match(line)
        if match:
            file_path = match.group(1).strip()
            line_number = int(match.group(2).strip())
            message_type = match.group(3).strip()
            message = match.group(4).strip()
            annotations.append({
                'file': file_path,
                'line': line_number,
                'message_type': message_type,
                'message': message
            })
    return annotations

def create_heterodata(cfg_data):
    """
    Convert CFG data into a HeteroData object for PyTorch Geometric.
    """
    data = HeteroData()
    nodes = cfg_data['nodes']
    edges = cfg_data['edges']

    # Create node features (e.g., label encoding)
    node_features = []
    node_labels = []
    node_indices = {}
    for node in nodes:
        node_id = node['id']
        label = node['label']
        # Simple feature: length of label
        feature = [len(label)]
        node_features.append(feature)
        node_indices[node_id] = len(node_indices)
        # Initialize labels to 0 (no annotation)
        node_labels.append(0)

    if not node_features:
        return None  # Skip if no nodes

    data['node'].x = torch.tensor(node_features, dtype=torch.float)
    data['node'].y = torch.tensor(node_labels, dtype=torch.long)

    # Create edge index
    edge_index = [[], []]
    for edge in edges:
        source = node_indices.get(edge['source'])
        target = node_indices.get(edge['target'])
        if source is not None and target is not None:
            edge_index[0].append(source)
            edge_index[1].append(target)

    if not edge_index[0]:
        return None  # Skip if no edges

    data['node', 'to', 'node'].edge_index = torch.tensor(edge_index, dtype=torch.long)
    return data

def label_nodes(data, cfg_data, annotations):
    """
    Label nodes in the data object based on warnings.
    """
    node_labels = data['node'].y.numpy()
    nodes = cfg_data['nodes']
    node_line_map = {}
    for idx, node in enumerate(nodes):
        line_number = node.get('line', None)
        if line_number is not None:
            node_line_map[line_number] = idx

    # Build a set of annotation line numbers
    annotation_lines = set()
    for annotation in annotations:
        if os.path.abspath(annotation['file']) == os.path.abspath(cfg_data['java_file']):
            annotation_lines.add(annotation['line'])

    # Label nodes
    for line_number in annotation_lines:
        node_idx = node_line_map.get(line_number)
        if node_idx is not None:
            node_labels[node_idx] = 1  # Mark node for annotation

    data['node'].y = torch.tensor(node_labels, dtype=torch.long)

# Define the HGT model
class HGTModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, num_layers, metadata):
        super(HGTModel, self).__init__()
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            conv = HGTConv(
                in_channels=in_channels,
                out_channels=hidden_channels,
                metadata=metadata,
                num_heads=num_heads
            )
            self.convs.append(conv)
        self.fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)
            # Apply activation function
            x_dict = {key: torch.relu(x) for key, x in x_dict.items()}
        # Output layer
        out = self.fc(x_dict['node'])
        return out

# Main training loop
def main():
    # Load data
    graphs = load_data()
    if not graphs:
        print("No data available for training.")
        return

    # Split data into training and validation sets
    random.shuffle(graphs)
    train_graphs, val_graphs = train_test_split(graphs, test_size=0.2, random_state=42)

    # Create data loaders
    train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_graphs, batch_size=16, shuffle=False)

    # Get metadata from a sample graph
    metadata = graphs[0].metadata()
    in_channels = graphs[0]['node'].x.size(-1)
    out_channels = 2  # Binary classification (annotate or not)

    # Define model, loss function, and optimizer
    model = HGTModel(
        in_channels=in_channels,
        hidden_channels=64,
        out_channels=out_channels,
        num_heads=2,
        num_layers=2,
        metadata=metadata
    )
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.005)

    best_val_loss = float('inf')
    num_epochs = 20

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for data in train_loader:
            data = data.to(device)
            optimizer.zero_grad()
            out = model(data.x_dict, data.edge_index_dict)
            loss = criterion(out, data['node'].y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * data['node'].num_nodes

        avg_loss = total_loss / len(train_loader.dataset)
        val_loss = evaluate(model, val_loader, criterion, device)

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}")

        # Save the best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_path = os.path.join(models_dir, 'best_model.pth')
            torch.save(model.state_dict(), model_path)
            print(f"New best model saved with validation loss {val_loss:.4f}")

def evaluate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total_nodes = 0
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            out = model(data.x_dict, data.edge_index_dict)
            loss = criterion(out, data['node'].y)
            total_loss += loss.item() * data['node'].num_nodes
            pred = out.argmax(dim=1)
            correct += (pred == data['node'].y).sum().item()
            total_nodes += data['node'].num_nodes
    avg_loss = total_loss / len(loader.dataset)
    accuracy = correct / total_nodes
    return avg_loss

if __name__ == '__main__':
    main()
