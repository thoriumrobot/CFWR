# causal_model.py

import os
import json
import numpy as np
import pandas as pd
import networkx as nx
import joblib
import subprocess
import re
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.ensemble import GradientBoostingClassifier
try:
    from dowhy import CausalModel  # type: ignore
    DOWHY_AVAILABLE = True
except Exception:
    CausalModel = None
    DOWHY_AVAILABLE = False

from cfg import generate_control_flow_graphs, save_cfgs

# Directory paths
cfg_output_dir = os.environ.get("CFG_OUTPUT_DIR", "cfg_output")
slices_dir = os.environ.get("SLICES_DIR", "slices")
# If SLICES_DIR points to a base directory, look for slicer-specific augmented directories
if not os.path.exists(slices_dir) or not any(f.endswith('.java') for f in os.listdir(slices_dir) if os.path.isfile(os.path.join(slices_dir, f))):
    # Look for augmented directories
    base_dir = os.path.dirname(slices_dir) if os.path.dirname(slices_dir) else "."
    for slicer in ['wala', 'specimin']:
        aug_dir = os.path.join(base_dir, f"slices_aug_{slicer}")
        if os.path.exists(aug_dir):
            slices_dir = aug_dir
            break
index_checker_cp = os.environ.get("CHECKERFRAMEWORK_CP", "")
models_dir = os.environ.get("MODELS_DIR", "models")

if not os.path.exists(models_dir):
    os.makedirs(models_dir)

def iter_java_files(root_dir):
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f.endswith('.java'):
                yield os.path.join(root, f)

def ensure_cfg(java_file):
    base = os.path.splitext(os.path.basename(java_file))[0]
    cfg_dir = os.path.join(cfg_output_dir, base)
    if not os.path.exists(cfg_dir) or not any(name.endswith('.json') for name in os.listdir(cfg_dir)):
        cfgs = generate_control_flow_graphs(java_file, cfg_output_dir)
        save_cfgs(cfgs, cfg_dir)

def load_data():
    """
    Load CFGs and prepare data for causal modeling.
    """
    data_records = []
    for java_file_path in iter_java_files(slices_dir):
        ensure_cfg(java_file_path)
        warnings_list = run_index_checker(java_file_path)
        annotations = parse_warnings(warnings_list)
        method_cfgs = load_cfgs(java_file_path)
        for cfg_data in method_cfgs:
            records = extract_features_and_labels(cfg_data, annotations)
            data_records.extend(records)
    return pd.DataFrame(data_records)

def run_index_checker(java_file):
    """
    Run the Checker Framework's Index Checker on the given Java file and capture warnings.
    """
    # Construct the command to run the Index Checker
    command = ['javac']
    if index_checker_cp:
        command += ['-cp', index_checker_cp]
    command += ['-processor', 'org.checkerframework.checker.index.IndexChecker', java_file]
    result = subprocess.run(command, capture_output=True, text=True)
    warnings_output = result.stderr  # Warnings are typically output to stderr
    return warnings_output

def parse_warnings(warnings_output):
    """
    Parse the warnings generated by Index Checker to identify nodes for annotations.
    """
    pattern = re.compile(r'^(.*\.java):(\d+):\s*(error|warning):\s*(.*)$', re.MULTILINE)
    annotations = []  # List of dictionaries with file, line, and message

    for match in pattern.finditer(warnings_output):
        file_path = match.group(1).strip()
        line_number = int(match.group(2).strip())
        message_type = match.group(3).strip()
        message = match.group(4).strip()
        annotations.append({
            'file': file_path,
            'line': line_number,
            'message_type': message_type,
            'message': message
        })
    return annotations

def load_cfgs(java_file):
    """
    Load the saved CFGs for a given Java file.
    """
    method_cfgs = []
    java_file_name = os.path.splitext(os.path.basename(java_file))[0]
    cfg_dir = os.path.join(cfg_output_dir, java_file_name)
    if os.path.exists(cfg_dir):
        for cfg_file in os.listdir(cfg_dir):
            if cfg_file.endswith('.json'):
                cfg_file_path = os.path.join(cfg_dir, cfg_file)
                with open(cfg_file_path, 'r') as f:
                    cfg_data = json.load(f)
                    # Add method name to cfg_data for identification
                    cfg_data['method_name'] = os.path.splitext(cfg_file)[0]
                    cfg_data['java_file'] = java_file
                    method_cfgs.append(cfg_data)
    else:
        print(f"CFG directory {cfg_dir} does not exist for Java file {java_file}")
    return method_cfgs

def extract_features_and_labels(cfg_data, annotations):
    """
    Extract features and labels from CFG data and annotations for causal modeling.
    """
    records = []
    nodes = cfg_data['nodes']
    edges = cfg_data['edges']
    method_name = cfg_data['method_name']
    java_file = cfg_data['java_file']
    # Build a set of annotation line numbers
    annotation_lines = set()
    for annotation in annotations:
        if os.path.abspath(annotation['file']) == os.path.abspath(java_file):
            annotation_lines.add(annotation['line'])
    # Create a mapping from node IDs to line numbers (if available)
    node_line_map = {}
    for node in nodes:
        node_id = node['id']
        line_number = node.get('line', None)
        if line_number is not None:
            node_line_map[node_id] = line_number
    for node in nodes:
        node_id = node['id']
        label = node['label']
        # Extract features from the node
        features = {
            'node_id': node_id,
            'method_name': method_name,
            'java_file': java_file,
            'label_length': len(label),
            'label': label,
        }
        # Degree features
        in_degree = 0
        out_degree = 0
        for edge in edges:
            if edge['target'] == node_id:
                in_degree += 1
            if edge['source'] == node_id:
                out_degree += 1
        features['in_degree'] = in_degree
        features['out_degree'] = out_degree
        # Line number (if available)
        line_number = node_line_map.get(node_id, None)
        features['line_number'] = line_number
        # Target variable: whether annotation is needed
        if line_number in annotation_lines:
            features['needs_annotation'] = 1
        else:
            features['needs_annotation'] = 0
        records.append(features)
    return records

def main():
    # Load data
    data = load_data()
    if data.empty:
        print("No data available for training.")
        return
    
    print(f"Loaded {len(data)} data points for causal model training")
    
    # Preprocess data
    data = preprocess_data(data)
    
    # Skip complex DoWhy causal inference and focus on predictive modeling
    print("Using simplified causal model approach (predictive classifier only)")
    
    # Use the causal model to predict where annotations should be placed
    data['predicted_annotation'], clf = predict_annotations(data)
    
    # Evaluate the model
    accuracy = accuracy_score(data['needs_annotation'], data['predicted_annotation'])
    f1 = f1_score(data['needs_annotation'], data['predicted_annotation'])
    print(f"Causal model accuracy: {accuracy:.4f}, F1-score: {f1:.4f}")
    
    # Save predictive classifier for inference
    clf_path = os.path.join(models_dir, 'causal_clf.joblib')
    joblib.dump(clf, clf_path)
    print(f"Predictive classifier saved at {clf_path}")
    
    # Print feature importance if available
    if hasattr(clf, 'feature_importances_'):
        feature_names = ['label_length', 'in_degree', 'out_degree', 'label_encoded', 'line_number']
        importances = clf.feature_importances_
        print("\nFeature importance:")
        for name, importance in zip(feature_names, importances):
            print(f"  {name}: {importance:.4f}")

def preprocess_data(data):
    """
    Preprocess data for causal modeling.
    """
    # Encode categorical variables
    data['label_encoded'] = data['label'].astype('category').cat.codes
    # Fill missing values
    data = data.fillna(0)
    return data

def define_causal_model(data):
    """
    Define the causal model using DoWhy.
    """
    # Specify the causal graph
    # Assuming the following relationships:
    # - Features affect whether an annotation is needed
    model = CausalModel(
        data=data,
        treatment=['label_length', 'in_degree', 'out_degree', 'label_encoded', 'line_number'],
        outcome='needs_annotation',
        graph="digraph{"
              "label_length -> needs_annotation;"
              "in_degree -> needs_annotation;"
              "out_degree -> needs_annotation;"
              "label_encoded -> needs_annotation;"
              "line_number -> needs_annotation;"
              "}",
    )
    return model

def predict_annotations(data):
    """
    Use the causal model to predict where annotations should be placed.
    """
    # Features used for prediction
    features = ['label_length', 'in_degree', 'out_degree', 'label_encoded', 'line_number']
    X = data[features]
    y = data['needs_annotation']
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    # Train a classifier
    clf = GradientBoostingClassifier()
    clf.fit(X_train, y_train)
    # Predict on the full dataset
    y_pred = clf.predict(X)
    return y_pred, clf

if __name__ == '__main__':
    main()
