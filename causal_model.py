# causal_model.py

import os
import json
import numpy as np
import pandas as pd
import networkx as nx
import joblib
import subprocess
import re
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.ensemble import GradientBoostingClassifier
import dowhy
from dowhy import CausalModel

# Directory paths
cfg_output_dir = "cfg_output"  # Directory where CFGs are saved
slices_dir = "slices"  # Directory containing Java code slices
index_checker_path = "path/to/checker-framework/checker/dist/checker.jar"  # Replace with actual path
models_dir = "models"  # Directory to save models

if not os.path.exists(models_dir):
    os.makedirs(models_dir)

def load_data():
    """
    Load CFGs and prepare data for causal modeling.
    """
    data_records = []
    for java_file in os.listdir(slices_dir):
        if java_file.endswith(".java"):
            java_file_path = os.path.join(slices_dir, java_file)
            # Run Index Checker to get warnings
            warnings_list = run_index_checker(java_file_path)
            annotations = parse_warnings(warnings_list)
            # Load CFGs
            method_cfgs = load_cfgs(java_file_path)
            for cfg_data in method_cfgs:
                # Extract features and labels
                records = extract_features_and_labels(cfg_data, annotations)
                data_records.extend(records)
    return pd.DataFrame(data_records)

def run_index_checker(java_file):
    """
    Run the Checker Framework's Index Checker on the given Java file and capture warnings.
    """
    # Construct the command to run the Index Checker
    command = [
        'javac',
        '-cp', index_checker_path,
        '-processor', 'org.checkerframework.checker.index.IndexChecker',
        java_file
    ]
    result = subprocess.run(command, capture_output=True, text=True)
    warnings_output = result.stderr  # Warnings are typically output to stderr
    return warnings_output

def parse_warnings(warnings_output):
    """
    Parse the warnings generated by Index Checker to identify nodes for annotations.
    """
    pattern = re.compile(r'^(.*\.java):(\d+):\s*(error|warning):\s*(.*)$', re.MULTILINE)
    annotations = []  # List of dictionaries with file, line, and message

    for match in pattern.finditer(warnings_output):
        file_path = match.group(1).strip()
        line_number = int(match.group(2).strip())
        message_type = match.group(3).strip()
        message = match.group(4).strip()
        annotations.append({
            'file': file_path,
            'line': line_number,
            'message_type': message_type,
            'message': message
        })
    return annotations

def load_cfgs(java_file):
    """
    Load the saved CFGs for a given Java file.
    """
    method_cfgs = []
    java_file_name = os.path.splitext(os.path.basename(java_file))[0]
    cfg_dir = os.path.join(cfg_output_dir, java_file_name)
    if os.path.exists(cfg_dir):
        for cfg_file in os.listdir(cfg_dir):
            if cfg_file.endswith('.json'):
                cfg_file_path = os.path.join(cfg_dir, cfg_file)
                with open(cfg_file_path, 'r') as f:
                    cfg_data = json.load(f)
                    # Add method name to cfg_data for identification
                    cfg_data['method_name'] = os.path.splitext(cfg_file)[0]
                    cfg_data['java_file'] = java_file
                    method_cfgs.append(cfg_data)
    else:
        print(f"CFG directory {cfg_dir} does not exist for Java file {java_file}")
    return method_cfgs

def extract_features_and_labels(cfg_data, annotations):
    """
    Extract features and labels from CFG data and annotations for causal modeling.
    """
    records = []
    nodes = cfg_data['nodes']
    edges = cfg_data['edges']
    method_name = cfg_data['method_name']
    java_file = cfg_data['java_file']
    # Build a set of annotation line numbers
    annotation_lines = set()
    for annotation in annotations:
        if os.path.abspath(annotation['file']) == os.path.abspath(java_file):
            annotation_lines.add(annotation['line'])
    # Create a mapping from node IDs to line numbers (if available)
    node_line_map = {}
    for node in nodes:
        node_id = node['id']
        line_number = node.get('line', None)
        if line_number is not None:
            node_line_map[node_id] = line_number
    for node in nodes:
        node_id = node['id']
        label = node['label']
        # Extract features from the node
        features = {
            'node_id': node_id,
            'method_name': method_name,
            'java_file': java_file,
            'label_length': len(label),
            'label': label,
        }
        # Degree features
        in_degree = 0
        out_degree = 0
        for edge in edges:
            if edge['target'] == node_id:
                in_degree += 1
            if edge['source'] == node_id:
                out_degree += 1
        features['in_degree'] = in_degree
        features['out_degree'] = out_degree
        # Line number (if available)
        line_number = node_line_map.get(node_id, None)
        features['line_number'] = line_number
        # Target variable: whether annotation is needed
        if line_number in annotation_lines:
            features['needs_annotation'] = 1
        else:
            features['needs_annotation'] = 0
        records.append(features)
    return records

def main():
    # Load data
    data = load_data()
    if data.empty:
        print("No data available for training.")
        return
    # Preprocess data
    data = preprocess_data(data)
    # Define causal model
    model = define_causal_model(data)
    # Identify causal effect
    identified_estimand = model.identify_effect()
    print("Identified estimand:")
    print(identified_estimand)
    # Estimate causal effect
    estimate = model.estimate_effect(identified_estimand,
                                     method_name="backdoor.propensity_score_matching")
    print("Causal effect estimate:")
    print(estimate)
    # Refute the estimate
    refute = model.refute_estimate(identified_estimand, estimate,
                                   method_name="placebo_treatment_refuter")
    print("Refutation result:")
    print(refute)
    # Use the causal model to predict where annotations should be placed
    data['predicted_annotation'] = predict_annotations(data)
    # Evaluate the model
    accuracy = accuracy_score(data['needs_annotation'], data['predicted_annotation'])
    f1 = f1_score(data['needs_annotation'], data['predicted_annotation'])
    print(f"Model accuracy: {accuracy:.4f}, F1-score: {f1:.4f}")
    # Save the model
    model_path = os.path.join(models_dir, 'causal_model.joblib')
    joblib.dump(model, model_path)
    print(f"Causal model saved at {model_path}")

def preprocess_data(data):
    """
    Preprocess data for causal modeling.
    """
    # Encode categorical variables
    data['label_encoded'] = data['label'].astype('category').cat.codes
    # Fill missing values
    data = data.fillna(0)
    return data

def define_causal_model(data):
    """
    Define the causal model using DoWhy.
    """
    # Specify the causal graph
    # Assuming the following relationships:
    # - Features affect whether an annotation is needed
    model = CausalModel(
        data=data,
        treatment=['label_length', 'in_degree', 'out_degree', 'label_encoded', 'line_number'],
        outcome='needs_annotation',
        graph="digraph{"
              "label_length -> needs_annotation;"
              "in_degree -> needs_annotation;"
              "out_degree -> needs_annotation;"
              "label_encoded -> needs_annotation;"
              "line_number -> needs_annotation;"
              "}",
    )
    return model

def predict_annotations(data):
    """
    Use the causal model to predict where annotations should be placed.
    """
    # Features used for prediction
    features = ['label_length', 'in_degree', 'out_degree', 'label_encoded', 'line_number']
    X = data[features]
    y = data['needs_annotation']
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    # Train a classifier
    clf = GradientBoostingClassifier()
    clf.fit(X_train, y_train)
    # Predict on the full dataset
    y_pred = clf.predict(X)
    return y_pred

if __name__ == '__main__':
    main()
