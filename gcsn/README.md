How to use (quick)

Save the three files next to your dataset.

Prepare PyG Data graphs with x, edge_index, y, train_mask/val_mask/test_mask.

Train:

python train_gcsn.py \
  --data_dir ./data_cfg_pt \
  --train_glob "train_*.pt" \
  --val_glob "val_*.pt" \
  --test_glob "test_*.pt" \
  --epochs 120 --batch_size 1 --amp


This writes ckpt_gcsn/best.pt and feature_keep_probs.json.

Predict (and export evidence):

python predict_gcsn.py \
  --ckpt ckpt_gcsn/best.pt \
  --data ./data_cfg_pt/test_000.pt \
  --out_dir preds_test \
  --edge_threshold 0.6


Per-graph .pt files contain node predictions plus the indices of kept edges, i.e., your minimal explanatory subgraph under the learned mask.

A model built for â€œall-or-nothingâ€ relevance
Intuition

Given a CFG 
ğº
=
(
ğ‘‰
,
ğ¸
)
G=(V,E) with node features 
ğ‘¥
ğ‘£
x
v
	â€‹

 and target types 
ğ‘¦
ğ‘£
y
v
	â€‹

, suppose there exists a minimal deterministic evidence set 
ğ‘†
â‹†
âŠ†
ğ¹
S
â‹†
âŠ†F (features/subgraph) such that

Pr
â¡
(
ğ‘¦
ğ‘£
âˆ£
ğº
,
ğ‘‹
)
=
1
{
ğ‘“
(
ğ‘‹
ğ‘†
â‹†
,
ğº
ğ‘†
â‹†
)
=
type
}
.
Pr(y
v
	â€‹

âˆ£G,X)=1{f(X
S
â‹†
	â€‹

,G
S
â‹†
	â€‹

)=type}.

Then the right bias is hard sparsity on features and subgraph structure, not smooth shrinkage. We want the network to choose a tiny subgraph and a tiny set of features that suffice to determine the type, and to be penalized for using anything else.

Architecture: Gated Causal Subgraph Network (GCSN)

Components (all learned end-to-end):

Per-feature L0 gates (Concrete/L0)
For each node feature channel 
ğ‘‘
d, learn 
ğ‘§
ğ‘‘
âˆˆ
{
0
,
1
}
z
d
	â€‹

âˆˆ{0,1} (relaxed to the Concrete distribution during training) and mask features by 
ğ‘¥
(
ğ‘š
ğ‘
ğ‘ 
ğ‘˜
ğ‘’
ğ‘‘
)
=
ğ‘§
âŠ™
ğ‘¥
x
(masked)
=zâŠ™x.

Parameterization: logits 
ğ›¼
ğ‘‘
Î±
d
	â€‹

; sample 
ğ‘¢
â€‰â£
âˆ¼
â€‰â£
Uniform
(
0
,
1
)
uâˆ¼Uniform(0,1), 
ğ‘ 
=
ğœ
(
(
log
â¡
ğ‘¢
âˆ’
log
â¡
(
1
âˆ’
ğ‘¢
)
+
ğ›¼
ğ‘‘
)
/
ğœ
)
s=Ïƒ((loguâˆ’log(1âˆ’u)+Î±
d
	â€‹

)/Ï„) with temperature 
ğœ
â†“
0
Ï„â†“0; straight-through at test time.

Loss term: 
ğœ†
ğ‘“
âˆ‘
ğ‘‘
ğ¸
[
ğ‘§
ğ‘‘
]
Î»
f
	â€‹

âˆ‘
d
	â€‹

E[z
d
	â€‹

] (feature sparsity).

Edge/Node hard subgraph gate (Top-k straight-through)
Compute importance scores 
ğ‘
ğ‘’
a
e
	â€‹

 for edges and 
ğ‘
ğ‘£
a
v
	â€‹

 for nodes (via small MLPs on edge/node features + local context). Keep only the top-k edges/nodes (straight-through one-hot), zero the rest. This produces a selected subgraph 
ğº
~
G
~
.

Loss term: 
ğœ†
ğ‘”
(
âˆ£
ğ¸
(
ğº
~
)
âˆ£
+
ğ›¾
âˆ£
ğ‘‰
(
ğº
~
)
âˆ£
)
Î»
g
	â€‹

(âˆ£E(
G
~
)âˆ£+Î³âˆ£V(
G
~
)âˆ£).

Relational message passing only on the selected subgraph
Any standard GNN (e.g., GAT/GINE) runs on 
ğº
~
G
~
 with masked features. This encodes the bias that only selected evidence can influence prediction.

Type head
Per-node classifier to predict the Checker-style type label (or a multi-label vector of qualifiers).

Stability & identifiability auxiliary terms

Counterfactual stability: train on paired program variants 
(
ğº
,
ğº
â€²
)
(G,G
â€²
) produced by semantics-preserving rewrites; enforce 
ğ‘
(
ğ‘¦
âˆ£
ğº
~
,
ğ‘‹
~
)
â‰ˆ
ğ‘
(
ğ‘¦
âˆ£
ğº
~
â€²
,
ğ‘‹
~
â€²
)
p(yâˆ£
G
~
,
X
~
)â‰ˆp(yâˆ£
G
~
â€²
,
X
~
â€²
) with KL penalty.

Minimality via MDL: add 
ğ›½
â€‰â£
â‹…
â€‰â£
codelen
(
ğº
~
,
ğ‘§
)
Î²â‹…codelen(
G
~
,z) (implemented as sparsity + entropy penalties) to prefer smaller explanations.

De-correlation (DeCov): penalize covariance of hidden features to avoid â€œusing many weakly correlated featuresâ€ when a single decisive one exists.

Training objective

ğ¿
=
CE
(
ğ‘¦
,
ğ‘¦
^
)
âŸ
accuracy
+
ğœ†
ğ‘“
âˆ‘
ğ‘‘
ğ¸
[
ğ‘§
ğ‘‘
]
+
ğœ†
ğ‘”
(
âˆ£
ğ¸
(
ğº
~
)
âˆ£
+
ğ›¾
âˆ£
ğ‘‰
(
ğº
~
)
âˆ£
)
+
ğœ†
ğ‘ 
KL
â¡
(
ğ‘
ğœƒ
(
ğ‘¦
âˆ£
ğº
)
,
ğ‘
ğœƒ
(
ğ‘¦
âˆ£
ğº
â€²
)
)
+
ğœ†
ğ‘‘
ğ‘
â€‰
DeCov
.
L=
accuracy
CE(y,
y
^
	â€‹

)
	â€‹

	â€‹

+Î»
f
	â€‹

d
âˆ‘
	â€‹

E[z
d
	â€‹

]+Î»
g
	â€‹

(âˆ£E(
G
~
)âˆ£+Î³âˆ£V(
G
~
)âˆ£)+Î»
s
	â€‹

KL(p
Î¸
	â€‹

(yâˆ£G),p
Î¸
	â€‹

(yâˆ£G
â€²
))+Î»
dc
	â€‹

DeCov.

Why this is â€œmathematically rightâ€ here
If 
ğ‘†
â‹†
S
â‹†
 is a Markov blanket of the label (deterministic or near-deterministic), then any superset predicts but is non-minimal; any subset fails. Under standard L0 recovery conditions (restricted strong convexity / identifiability) and sufficient augmentation coverage, the global minimizer of 
ğ¿
L selects 
ğ‘†
â‹†
S
â‹†
 (or an equivalent minimal set) with probability 
â†’
1
â†’1 as data grows. In practice, the Concrete/L0 + top-k gates are the computationally tractable surrogate for exact subset search.

1) Where this model can stumble â€” and how to harden it

A. Gate collapse (too sparse or too dense).

Symptom: training loss stalls; masks go all 0s or nearly all 1s.

Fixes:

Warm-start gates: small positive logit init for features and edges (e.g., init_logit=+0.5).

Anneal L0 temperature Ï„ from 2/3 â†’ 0.1 over epochs; linearly/cosine.

Budget scheduling: start with larger edge k_ratio (e.g., 0.4) then decay to target (e.g., 0.1).

Use loss-adaptive gate penalties (increase Î» if mask density exceeds target, decrease if too small).

B. Non-differentiable edge pruning kills learning signal.

Symptom: if edges are hard-dropped by reconstructing edge_index, gradients wonâ€™t reach the scorer.

Fix: Keep full edge_index; pass the (straight-through) mask as a continuous edge feature into an attention GNN (e.g., GATv2Conv(edge_dim=1)), so attention multiplies by the mask â†’ gradients flow to the gate scorer.

C. Shortcut leakage via label-adjacent features.

Symptom: the model â€œreadsâ€ target-only features (e.g., oracle types in scope) instead of CFG evidence.

Fix: audit features; if needed, block gradients through â€œlabel-adjacentâ€ channels (stop-grad), or place them behind higher Î» L0 gates, or drop them entirely. Add counterfactuals: shuffle/perturb those channels while holding CFG constant; penalize prediction change.

D. Class imbalance.

Symptom: great loss, poor minority-type recall.

Fix: class-weighted CE (weight=1/log(freq+Î´)), macro-F1 early-stopping, and minority-oversampling of graphs/nodes.

E. Oversmoothing on long CFG paths / large graphs.

Symptom: depth hurts performance; signals blur.

Fix: shallow (2â€“3) GATv2 layers, residual MLP head, and the edge gate itself (selective propagation) reduce smoothing. Optionally, add PairNorm or Jumping-Knowledge.

F. Instability across semantics-preserving rewrites.

Symptom: predictions flip after variable-renames or commutations.

Fix: train-time equivariance set: batch pairs of augmented graphs; add KL stability regularizer between outputs. Also include DeCov on hidden activations to avoid reliance on incidental correlations.

G. OOD projects / symbol sets.

Symptom: poor generalization to unseen APIs/coding styles.

Fix: mixed-project training, token bucketing & hashing for rare/OOV identifiers, and input dropout on identifier channels.

H. Runtime & memory (big E, big N).

Fix: neighbor sampling (mini-batch by graph), float16 AMP, gradient clipping, and mask-aware pruning at inference (threshold the learned edge mask to actually shrink the graph before the forward).

2) Complete implementation (PyTorch Geometric)

The scripts assume you already have a dataset that yields torch_geometric.data.Data objects with:

x [N,D] node features,

edge_index [2,E] CFG edges,

y [N] integer node labels,

boolean masks: train_mask, val_mask, test_mask.
(If you only have per-graph labels, you can adapt the reduction in the loss/pred blocks.)